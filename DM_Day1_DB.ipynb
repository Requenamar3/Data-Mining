{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Requenamar3/Data-Mining/blob/main/DM_Day1_DB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
          "showTitle": false,
          "title": ""
        },
        "id": "p2OlLc19UT93"
      },
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
        "\n",
        "This notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6482be4c-f067-47c9-b0ac-35c938b94601",
          "showTitle": false,
          "title": ""
        },
        "id": "FV661n_lUT94",
        "outputId": "fb3d518b-60c3-41d1-a966-e753ddf2611f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'spark' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-142f7f7e714a>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# The applied options are for CSV files. For other file types, these will be ignored.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_row_is_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
          ]
        }
      ],
      "source": [
        "# File location and type\n",
        "file_location = \"/FileStore/tables/OnlineRetail.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b2a31f67-e795-4be9-ab9e-6bf965b2a4eb",
          "showTitle": false,
          "title": ""
        },
        "id": "0TtUUfiAUT95"
      },
      "outputs": [],
      "source": [
        "display(df.limit(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "1ba90498-1191-4b10-bd4e-bcba1b55edbb",
          "showTitle": false,
          "title": ""
        },
        "id": "-sFAZEcnUT95"
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "45c20d2a-8174-40ca-bc37-633e70f420ab",
          "showTitle": false,
          "title": ""
        },
        "id": "6fK9QW5rUT96"
      },
      "outputs": [],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "c1ed9075-9a05-453a-886b-defe0634a1ed",
          "showTitle": false,
          "title": ""
        },
        "id": "At2cnCabUT96"
      },
      "outputs": [],
      "source": [
        "# This line of code is a \"trick\"... it will convert your Spark DF - into a Pandas DF so normal \"Python\" rules apply\n",
        "df = df.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5613a2cb-1325-42ce-8091-081f73b74b81",
          "showTitle": false,
          "title": ""
        },
        "id": "5sFFjA3vUT96"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b1ef6cab-b32c-4db4-8d5d-765a73758398",
          "showTitle": false,
          "title": ""
        },
        "id": "ycJuLdanUT96"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "f3f977fb-c805-4eec-896d-5e9600674da4",
          "showTitle": false,
          "title": ""
        },
        "id": "Sf5Zb99dUT96"
      },
      "outputs": [],
      "source": [
        "# Drop Null values\n",
        "df.dropna(subset=['CustomerID'], inplace=True)\n",
        "\n",
        "# Keep positive quantity values\n",
        "df = df.loc[df['Quantity'] > 0]\n",
        "\n",
        "# Remove rows with negative price\n",
        "df = df.loc[df['UnitPrice'] > 0]\n",
        "\n",
        "# Ignore the records for the incomplete month\n",
        "df = df.loc[df['InvoiceDate'] < '2011-12-01']\n",
        "\n",
        "# Calculate total sales\n",
        "df['Sales'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Create a dataframe with orders\n",
        "df_orders = df.groupby(['CustomerID',\n",
        "                        'InvoiceNo']).agg({'Sales': sum,\n",
        "                                           'InvoiceDate': max})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "a7bfc5a6-47f0-4400-9905-eb8978138e0b",
          "showTitle": false,
          "title": ""
        },
        "id": "8pgFTCitUT96"
      },
      "outputs": [],
      "source": [
        "df_orders.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d3cf98aa-5d7b-4064-8f15-73a995bbdb49",
          "showTitle": false,
          "title": ""
        },
        "id": "kNMt-l0VUT96"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bd82bb99-1479-4d5c-be10-8c36df0f1d44",
          "showTitle": false,
          "title": ""
        },
        "id": "8kb57YyOUT97"
      },
      "outputs": [],
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"OnlineRetail_csv\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "implicitDf": true,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "b5f66379-6f7f-42ec-8e82-d0e0926a1721",
          "showTitle": false,
          "title": ""
        },
        "id": "5ctC_AhOUT97"
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "\n",
        "/* Query the created temp table in a SQL cell */\n",
        "\n",
        "select * from `OnlineRetail_csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "db9631f6-bb4a-42ca-8a3c-0d48af932331",
          "showTitle": false,
          "title": ""
        },
        "id": "K6PfwinFUT97"
      },
      "outputs": [],
      "source": [
        "# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n",
        "# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n",
        "# To do so, choose your table name and uncomment the bottom line.\n",
        "\n",
        "permanent_table_name = \"OnlineRetail_csv\"\n",
        "\n",
        "# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "mostRecentlyExecutedCommandWithImplicitDF": {
          "commandId": 1448331177537031,
          "dataframes": [
            "_sqldf"
          ]
        },
        "pythonIndentUnit": 4
      },
      "notebookName": "DM_Day1_DB",
      "widgets": {}
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}